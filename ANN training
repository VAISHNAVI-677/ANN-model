import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

# --------------------------
# 1. Data Loading
# --------------------------

def load_dataset(dataset_path, image_size):
    """Load dataset from directories and preprocess the images."""
    X, y = [], []
    class_labels = sorted(os.listdir(dataset_path))
    label_mapping = {class_name: idx for idx, class_name in enumerate(class_labels)}
    
    for class_name in class_labels:
        class_folder = os.path.join(dataset_path, class_name)
        if not os.path.isdir(class_folder):
            continue
        for file_name in os.listdir(class_folder):
            file_path = os.path.join(class_folder, file_name)
            try:
                # Open, resize and normalize the image
                img = Image.open(file_path).resize(image_size).convert("RGB")
                img_array = np.array(img) / 255.0  # Normalize to [0, 1]
                X.append(img_array)
                y.append(label_mapping[class_name])
            except Exception as e:
                print(f"Error loading image {file_path}: {e}")

    return np.array(X), np.array(y), label_mapping

# --------------------------
# 2. ANN Model
# --------------------------

class ANN:
    def _init_(self, input_size, hidden_size, output_size):
        # Initialize weights and biases
        self.weights1 = np.random.randn(input_size, hidden_size) * 0.01
        self.biases1 = np.zeros((1, hidden_size))
        self.weights2 = np.random.randn(hidden_size, output_size) * 0.01
        self.biases2 = np.zeros((1, output_size))

    def softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)
    def relu(self, z):
        return np.maximum(0, z)

    def relu_derivative(self, z):
        return np.where(z > 0, 1, 0)

    def forward(self, X):
        # Forward propagation
        self.z1 = np.dot(X, self.weights1) + self.biases1
        self.a1 = self.relu(self.z1)
        self.z2 = np.dot(self.a1, self.weights2) + self.biases2
        self.a2 = self.softmax(self.z2)
        return self.a2

    def backward(self, X, y, output, learning_rate):
        # One-hot encoding of labels
        m = y.shape[0]
        y_one_hot = np.zeros((m, output.shape[1]))
        y_one_hot[np.arange(m), y] = 1

        # Loss derivative
        dz2 = output - y_one_hot
        dw2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m

        # Backpropagate through hidden layer
        dz1 = np.dot(dz2, self.weights2.T) * self.relu_derivative(self.z1)
        dw1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m

        # Update weights and biases
        self.weights2 -= learning_rate * dw2
        self.biases2 -= learning_rate * db2
        self.weights1 -= learning_rate * dw1
        self.biases1 -= learning_rate * db1

    def compute_loss(self, y, output):
        m = y.shape[0]
        y_one_hot = np.zeros((m, output.shape[1]))
        y_one_hot[np.arange(m), y] = 1
        log_likelihood = -np.log(output[np.arange(m), y])
        loss = np.sum(log_likelihood) / m
        return loss
    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)

# --------------------------
# 3. Training the Model
# --------------------------

def train(model, X_train, y_train, epochs, learning_rate):
    for epoch in range(epochs):
        # Forward propagation
        output = model.forward(X_train)
        
        # Loss computation
        loss = model.compute_loss(y_train, output)
        
        # Backward propagation
        model.backward(X_train, y_train, output, learning_rate)
        
        # Print loss
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}")

# --------------------------
# 4. Main Program
# --------------------------

# Dataset path and parameters
dataset_path = r"C:\Users\ASUS\Downloads\Indian_Medicinal_Plantsextracted\Indian_Medicinal_Plants"  # Replace with the path to your dataset
image_size = (128, 128)     # Resize all images to 64x64
hidden_layer_size = 128   # Number of neurons in the hidden layer
learning_rate = 0.01
epochs = 100

# Load dataset
X, y, label_mapping = load_dataset(dataset_path, image_size)
num_classes = len(label_mapping)

# Flatten image data for input into ANN
X_flattened = X.reshape(X.shape[0], -1)
input_size = X_flattened.shape[1]

# Shuffle the data
indices = np.arange(X.shape[0])
np.random.shuffle(indices)
X_flattened, y = X_flattened[indices], y[indices]

# Train-test split
split_ratio = 0.8
split_idx = int(split_ratio * X_flattened.shape[0])
X_train, X_test = X_flattened[:split_idx], X_flattened[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# Initialize and train the model
model = ANN(input_size, hidden_layer_size, num_classes)
train(model, X_train, y_train, epochs, learning_rate)

# Test the model
predictions = model.predict(X_test)
accuracy = np.mean(predictions == y_test) * 100
print(f"Test Accuracy: {accuracy:.2f}%")

# --------------------------
# 5. Visualization
# --------------------------
visualize_predictions(X_test[:9].reshape(-1, 128, 128, 3), y_test[:9], predictions[:9], label_mapping)
